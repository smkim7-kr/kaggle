{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEV8tCydE9zs"
      },
      "source": [
        "# !pip install -q pytorch_toolbelt\n",
        "# !pip install -q torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0\n",
        "# !pip install -q git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "# !pip install -q pycocotools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZdmkm6yme68"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xif7iKbpsbrH",
        "outputId": "7703db12-4e86-4871-9811-ce60cc7e6fd9"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Python package for pre-processing \n",
        "from pycocotools.coco import COCO\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_toolbelt import losses as L\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensor\n",
        "# import RandAugment\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "from torchvision.transforms import transforms\n",
        "# from torchcontrib.optim import SWA\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "\n",
        "\n",
        "# Python package for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "plt.rcParams['axes.grid'] = False\n",
        "\n",
        "print('Pytorch version: {}'.format(torch.__version__))\n",
        "print('Is GPU available: {}'.format(torch.cuda.is_available()))\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "  print('The number of GPUs available: {}'.format(torch.cuda.device_count()))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "\n",
        "print('CPU count: {}'.format(os.cpu_count()))  # 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version: 1.8.0\n",
            "Is GPU available: True\n",
            "Tesla P100-PCIE-16GB\n",
            "The number of GPUs available: 1\n",
            "CPU count: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW6wr99nubZ8"
      },
      "source": [
        "batch_size = 8 # Mini-batch size\n",
        "num_epochs = 15\n",
        "learning_rate = 1e-02 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldCLgwXLvOAo"
      },
      "source": [
        "random_seed = 21\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed) \n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ffls0hfvSC-",
        "outputId": "d3d0030c-09de-41b2-89a2-7354a3367e5c"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/segment/data'\n",
        "anns_file_path = os.path.join(dataset_path, 'train.json')\n",
        "\n",
        "# Read annotations\n",
        "with open(anns_file_path, 'r') as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "categories = dataset['categories']\n",
        "anns = dataset['annotations']\n",
        "imgs = dataset['images']\n",
        "nr_cats = len(categories)\n",
        "nr_annotations = len(anns)\n",
        "nr_images = len(imgs)\n",
        "\n",
        "# Load categories and super categories\n",
        "cat_names = []\n",
        "super_cat_names = []\n",
        "super_cat_ids = {}\n",
        "super_cat_last_name = ''\n",
        "nr_super_cats = 0\n",
        "for cat_it in categories:\n",
        "    cat_names.append(cat_it['name'])\n",
        "    super_cat_name = cat_it['supercategory']\n",
        "    # Adding new supercat\n",
        "    if super_cat_name != super_cat_last_name:\n",
        "        super_cat_names.append(super_cat_name)\n",
        "        super_cat_ids[super_cat_name] = nr_super_cats\n",
        "        super_cat_last_name = super_cat_name\n",
        "        nr_super_cats += 1\n",
        "\n",
        "print('Number of super categories:', nr_super_cats)\n",
        "print('Number of categories:', nr_cats)\n",
        "print('Number of annotations:', nr_annotations)\n",
        "print('Number of images:', nr_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of super categories: 11\n",
            "Number of categories: 11\n",
            "Number of annotations: 21116\n",
            "Number of images: 2617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjVksQqUUlwP"
      },
      "source": [
        "# Count annotations\n",
        "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
        "for ann in anns:\n",
        "    cat_histogram[ann['category_id']] += 1\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
        "df = df.sort_values('Number of annotations', 0, False)\n",
        "\n",
        "sorted_temp_df = df.sort_index()\n",
        "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
        "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)\n",
        "sorted_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVIVLWiT_ceM"
      },
      "source": [
        "category_names = list(sorted_df.Categories)\n",
        "\n",
        "def get_classname(classID, cats):\n",
        "    for i in range(len(cats)):\n",
        "        if cats[i]['id']==classID:\n",
        "            return cats[i]['name']\n",
        "    return \"None\"\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"COCO format\"\"\"\n",
        "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.coco = COCO(data_dir)\n",
        "        \n",
        "    def __getitem__(self, index: int):\n",
        "        # Get the image_info using coco library\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "        image_infos = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # Load the image using opencv\n",
        "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
        "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        images /= 255.0\n",
        "        \n",
        "        if (self.mode in ('train', 'val')):\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
        "            anns = self.coco.loadAnns(ann_ids)\n",
        "            # print(\"image_infos['id'] : {}\".format(image_infos['id']) )\n",
        "            # Load the categories in a variable\n",
        "            cat_ids = self.coco.getCatIds()\n",
        "            cats = self.coco.loadCats(cat_ids)\n",
        "\n",
        "            # masks_size : height x width            \n",
        "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]), dtype=np.float32)\n",
        "  \n",
        "            # Background = 0, Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
        "            for i in range(len(anns)):\n",
        "                className = get_classname(anns[i]['category_id'], cats)\n",
        "                pixel_value = category_names.index(className)\n",
        "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
        "            \n",
        "\n",
        "            # We can use Albumentations for image & mask transformation(or augmentation)\n",
        "            if self.transform is not None:\n",
        "                transformed = self.transform(image=images, mask=masks)\n",
        "                images = transformed[\"image\"]\n",
        "                masks = transformed[\"mask\"]\n",
        "                masks =  masks.squeeze()\n",
        "            \n",
        "            return images, masks, image_infos\n",
        "        \n",
        "        if self.mode == 'test':            \n",
        "            if self.transform is not None:\n",
        "                transformed = self.transform(image=images)\n",
        "                images = transformed[\"image\"]\n",
        "            \n",
        "            return images, image_infos\n",
        "    \n",
        "    \n",
        "    def __len__(self) -> int:        \n",
        "        return len(self.coco.getImgIds())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVWIw1uwzUzo"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "\n",
        "class MultilabelBalancedRandomSampler(Sampler):\n",
        "    \"\"\"\n",
        "    MultilabelBalancedRandomSampler: Given a multilabel dataset of length n_samples and\n",
        "    number of classes n_classes, samples from the data with equal probability per class\n",
        "    effectively oversampling minority classes and undersampling majority classes at the\n",
        "    same time. Note that using this sampler does not guarantee that the distribution of\n",
        "    classes in the output samples will be uniform, since the dataset is multilabel and\n",
        "    sampling is based on a single class. This does however guarantee that all classes\n",
        "    will have at least batch_size / n_classes samples as batch_size approaches infinity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, indices=None, class_choice=\"least_sampled\"):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "            labels: a multi-hot encoding numpy array of shape (n_samples, n_classes)\n",
        "            indices: an arbitrary-length 1-dimensional numpy array representing a list\n",
        "            of indices to sample only from\n",
        "            class_choice: a string indicating how class will be selected for every\n",
        "            sample:\n",
        "                \"least_sampled\": class with the least number of sampled labels so far\n",
        "                \"random\": class is chosen uniformly at random\n",
        "                \"cycle\": the sampler cycles through the classes sequentially\n",
        "        \"\"\"\n",
        "        self.labels = labels\n",
        "        self.indices = indices\n",
        "        if self.indices is None:\n",
        "            self.indices = range(len(labels))\n",
        "\n",
        "        self.num_classes = self.labels.shape[1]\n",
        "\n",
        "        # List of lists of example indices per class\n",
        "        self.class_indices = []\n",
        "        for class_ in range(self.num_classes):\n",
        "            lst = np.where(self.labels[:, class_] == 1)[0]\n",
        "            lst = lst[np.isin(lst, self.indices)]\n",
        "            self.class_indices.append(lst)\n",
        "\n",
        "        self.counts = [0] * self.num_classes\n",
        "\n",
        "        assert class_choice in [\"least_sampled\", \"random\", \"cycle\"]\n",
        "        self.class_choice = class_choice\n",
        "        self.current_class = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.count >= len(self.indices):\n",
        "            raise StopIteration\n",
        "        self.count += 1\n",
        "        return self.sample()\n",
        "\n",
        "    def sample(self):\n",
        "        class_ = self.get_class()\n",
        "        class_indices = self.class_indices[class_]\n",
        "        chosen_index = np.random.choice(class_indices)\n",
        "        if self.class_choice == \"least_sampled\":\n",
        "            for class_, indicator in enumerate(self.labels[chosen_index]):\n",
        "                if indicator == 1:\n",
        "                    self.counts[class_] += 1\n",
        "        return chosen_index\n",
        "\n",
        "    def get_class(self):\n",
        "        if self.class_choice == \"random\":\n",
        "            class_ = random.randint(0, self.labels.shape[1] - 1)\n",
        "        elif self.class_choice == \"cycle\":\n",
        "            class_ = self.current_class\n",
        "            self.current_class = (self.current_class + 1) % self.labels.shape[1]\n",
        "        elif self.class_choice == \"least_sampled\":\n",
        "            min_count = self.counts[0]\n",
        "            min_classes = [0]\n",
        "            for class_ in range(1, self.num_classes):\n",
        "                if self.counts[class_] < min_count:\n",
        "                    min_count = self.counts[class_]\n",
        "                    min_classes = [class_]\n",
        "                if self.counts[class_] == min_count:\n",
        "                    min_classes.append(class_)\n",
        "            class_ = np.random.choice(min_classes)\n",
        "        return class_\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeOpfyXAzUKa"
      },
      "source": [
        "anns_file_path = os.path.join(dataset_path, 'train_all.json')\n",
        "\n",
        "# Read annotations\n",
        "with open(anns_file_path, 'r') as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "anns = dataset['annotations']\n",
        "category_info_tr = np.zeros((3272, 11))\n",
        "for n in range(len(anns)):\n",
        "    img_id = anns[n]['image_id']\n",
        "    cate_id = anns[n]['category_id']\n",
        "    category_info_tr[img_id][cate_id] += 1\n",
        "\n",
        "category_info_tr[category_info_tr>0] = 1\n",
        "\n",
        "# anns_file_path = os.path.join(dataset_path, 'train.json')\n",
        "\n",
        "# # Read annotations\n",
        "# with open(anns_file_path, 'r') as f:\n",
        "#     dataset = json.loads(f.read())\n",
        "\n",
        "# anns = dataset['annotations']\n",
        "# category_info_tr = np.zeros((2617, 11))\n",
        "# for n in range(len(anns)):\n",
        "#     img_id = anns[n]['image_id']\n",
        "#     cate_id = anns[n]['category_id']\n",
        "#     category_info_tr[img_id][cate_id] += 1\n",
        "\n",
        "# category_info_tr[category_info_tr>0] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIQtybXTAa3Q",
        "outputId": "ef135baf-fdd7-4855-bafe-0233841db71a"
      },
      "source": [
        "train_path = os.path.join(dataset_path, 'train.json')\n",
        "val_path = os.path.join(dataset_path, 'val.json')\n",
        "train_all_path = os.path.join(dataset_path, 'train_all.json')\n",
        "\n",
        "# collate_fn needs for batch\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_transform = A.Compose([  \n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    # A.OneOf([\n",
        "    #   A.RandomCrop(320, 320, p=1),\n",
        "    #   A.RandomCrop(480, 480, p=1),\n",
        "    #   A.RandomCrop(224, 224, p=1)\n",
        "    #   ], p=1),\n",
        "    A.RandomCrop(320, 320, p=1),\n",
        "    A.Resize(512, 512, p=1),\n",
        "    A.RandomBrightnessContrast(p=0.9),\n",
        "    A.RandomGamma(p=0.3),\n",
        "    A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, shift_limit=0.1, p=1),\n",
        "    A.Cutout(num_holes=1, max_h_size=80, max_w_size=80),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([  \n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "train_all_dataset = CustomDataset(data_dir=train_all_path, mode='train', transform=train_transform)\n",
        "# train_dataset, val_dataset = random_split(train_all_dataset, [2896, 376], generator=torch.Generator().manual_seed(random_seed))\n",
        "\n",
        "# train_dataset = CustomDataset(data_dir=train_path, mode='train', transform=train_transform)\n",
        "# val_dataset = CustomDataset(data_dir=val_path, mode='val', transform=val_transform)\n",
        "\n",
        "train_sampler = MultilabelBalancedRandomSampler(\n",
        "        category_info_tr, class_choice=\"least_sampled\"\n",
        "    )\n",
        "\n",
        "# DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_all_dataset, \n",
        "                                           batch_size=batch_size,\n",
        "                                           pin_memory=True,\n",
        "                                           sampler = train_sampler,\n",
        "                                           num_workers=4,\n",
        "                                           drop_last=True,\n",
        "                                           collate_fn=collate_fn)\n",
        "\n",
        "# val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "#                                          batch_size=batch_size,\n",
        "#                                          pin_memory=True,\n",
        "#                                          num_workers=4,\n",
        "#                                          collate_fn=collate_fn)\n",
        "val_loader = None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=4.76s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWMfF6dDEtVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd7d67b-d696-4782-c602-dd22bd8b922e"
      },
      "source": [
        "model_path = '/content/drive/MyDrive/segment/saved/realrealfinal/35ep_best_model_seresnext50_dsize.pt'\n",
        "\n",
        "# initialize the model\n",
        "model = smp.DeepLabV3Plus(encoder_name='tu-seresnext50_32x4d', classes=12, encoder_weights='imagenet').to(device)\n",
        "# load the saved best model\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "state_dict = checkpoint.state_dict()\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# model = smp.DeepLabV3Plus(encoder_name='tu-efficientnet_b3', classes=12, encoder_weights='imagenet').to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzJ-qNJCEw62"
      },
      "source": [
        "def train(num_epochs, model, data_loader, val_loader, criterion,  optimizer, scheduler, saved_dir, val_every, device):\n",
        "    print('Start training..')\n",
        "    best_loss = 9999999\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        trn_mIoU = []\n",
        "        trn_acc = []\n",
        "        model.zero_grad()       \n",
        "        for step, (images, masks, _) in enumerate(data_loader):\n",
        "            images = torch.stack(images)       # (batch, channel, height, width)\n",
        "            masks = torch.stack(masks).long()  # (batch, height, width)\n",
        "            masks_tensor = masks.view(images.shape[0], 1, images.shape[2], images.shape[3])\n",
        "            zeros = torch.zeros(images.shape[0], 12, images.shape[2], images.shape[3], dtype=masks.dtype)\n",
        "            masks = zeros.scatter_(1, masks_tensor, 1).to(device) \n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks) \n",
        "            loss = loss / 4\n",
        "            # optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "            # optimizer.step()\n",
        "            # print(model.parameters())\n",
        "            if (step+1) % 4 == 0:             \n",
        "              optimizer.step()                            \n",
        "              model.zero_grad()    \n",
        "\n",
        "            outputs = torch.argmax(outputs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "            masks = torch.argmax(masks.squeeze(), dim=1).detach().cpu().numpy()\n",
        "            res = label_accuracy_score(masks, outputs, n_class=12)\n",
        "            # tmIoU_list, tb_mIoU = mIoU(outputs, masks, smooth=1e-10, n_classes=12)\n",
        "            # acc = pixel_accuracy(outputs, masks)\n",
        "            trn_mIoU.append(res[2])\n",
        "            trn_acc.append(res[0])\n",
        "            \n",
        "            # print the loss at 20 step intervals.\n",
        "            if (step + 1) % 20 == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, lr: {}'.format(\n",
        "                    epoch+1, num_epochs, step+1, len(train_loader), loss.item(), optimizer.param_groups[0][\"lr\"]))\n",
        "        print('Epoch {} - mIoU: {:.4f}, acc: {:.4f}'.format(epoch+1, np.mean(trn_mIoU), np.mean(trn_acc)))\n",
        "   \n",
        "        scheduler.step()\n",
        "        if np.mean(trn_mIoU) > 0.68:\n",
        "          save_model(model, saved_dir, epoch+1)\n",
        "\n",
        "\n",
        "        # if (epoch + 1) % val_every == 0:\n",
        "        #     avrg_loss, vld_mIoU = validation(epoch + 1, model, val_loader, criterion, device)\n",
        "        #     if avrg_loss < best_loss and vld_mIoU > 0.48:\n",
        "        #         print('Best performance at epoch: {}'.format(epoch + 1))\n",
        "        #         print('Save model in', saved_dir)\n",
        "        #         best_loss = avrg_loss\n",
        "        #         save_model(model, saved_dir, epoch+1)\n",
        "        #     elif epoch+1>=30 and (epoch+1)%5==0:\n",
        "        #         save_model(model, saved_dir, epoch+1)\n",
        "\n",
        "    save_model(model, saved_dir, 'final')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Qaazg7E69u"
      },
      "source": [
        "def validation(epoch, model, data_loader, criterion, device):\n",
        "    print('Start validation #{}'.format(epoch))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        cnt = 0\n",
        "        vld_mIoU = []\n",
        "        vld_acc = []\n",
        "        for step, (images, masks, _) in enumerate(data_loader):\n",
        "            \n",
        "            images = torch.stack(images)       # (batch, channel, height, width)\n",
        "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
        "            masks_tensor = masks.view(images.shape[0], 1, images.shape[2], images.shape[3])\n",
        "            zeros = torch.zeros(images.shape[0], 12, images.shape[2], images.shape[3], dtype=masks.dtype)\n",
        "            masks = zeros.scatter_(1, masks_tensor, 1).to(device) \n",
        "            images, masks = images.to(device), masks.to(device)         \n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks) \n",
        "            total_loss += loss\n",
        "            cnt += 1\n",
        "           \n",
        "            outputs = torch.argmax(outputs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "            masks = torch.argmax(masks.squeeze(), dim=1).detach().cpu().numpy()\n",
        "            res = label_accuracy_score(masks, outputs, n_class=12)\n",
        "            # bmIoU_list, vb_mIoU = mIoU(outputs, masks, smooth=1e-10, n_classes=12)\n",
        "            # acc = pixel_accuracy(outputs, masks)\n",
        "            vld_mIoU.append(res[2])\n",
        "            vld_acc.append(res[0])\n",
        "            \n",
        "        avrg_loss = total_loss / cnt\n",
        "        print('Validation #{}  Average Loss: {:.4f}, mIoU: {:.4f}, acc: {:.4f}'.format(epoch, avrg_loss, np.mean(vld_mIoU), np.mean(vld_acc)))\n",
        "\n",
        "    return avrg_loss, np.mean(vld_mIoU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEgmQniSI2Xc"
      },
      "source": [
        "# define the evaluation function\n",
        "# https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/utils.py\n",
        "import numpy as np\n",
        "\n",
        "def _fast_hist(label_true, label_pred, n_class):\n",
        "    mask = (label_true >= 0) & (label_true < n_class)\n",
        "    hist = np.bincount(\n",
        "        n_class * label_true[mask].astype(int) +\n",
        "        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n",
        "    return hist\n",
        "\n",
        "\n",
        "def label_accuracy_score(label_trues, label_preds, n_class):\n",
        "    \"\"\"Returns accuracy score evaluation result.\n",
        "      - overall accuracy\n",
        "      - mean accuracy\n",
        "      - mean IU\n",
        "      - fwavacc\n",
        "    \"\"\"\n",
        "    hist = np.zeros((n_class, n_class))\n",
        "    for lt, lp in zip(label_trues, label_preds):\n",
        "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
        "    acc = np.diag(hist).sum() / hist.sum()\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
        "    acc_cls = np.nanmean(acc_cls)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        iu = np.diag(hist) / (\n",
        "            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n",
        "        )\n",
        "    mean_iu = np.nanmean(iu)\n",
        "    freq = hist.sum(axis=1) / hist.sum()\n",
        "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "    return acc, acc_cls, mean_iu, fwavacc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktxlRlASE8Vl"
      },
      "source": [
        "val_every = 1 \n",
        "\n",
        "saved_dir = '/content/drive/MyDrive/segment/saved/realrealfinal'\n",
        "if not os.path.isdir(saved_dir):                                                           \n",
        "    os.mkdir(saved_dir)\n",
        "    \n",
        "def save_model(model, saved_dir, epoch, file_name='best_model_eff3_dsize.pt'):\n",
        "    file_name = str(epoch) + \"ep_\" + file_name\n",
        "    check_point = {'net': model.state_dict()}\n",
        "    output_path = os.path.join(saved_dir, file_name)\n",
        "    torch.save(model, output_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAjWaPubOzx2"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "try:\n",
        "    from itertools import ifilterfalse\n",
        "except ImportError:  # py3k\n",
        "    from itertools import filterfalse\n",
        "\n",
        "eps = 1e-6\n",
        "\n",
        "\n",
        "def dice_round(preds, trues):\n",
        "    preds = preds.float()\n",
        "    return soft_dice_loss(preds, trues)\n",
        "\n",
        "\n",
        "def soft_dice_loss(outputs, targets, per_image=False):\n",
        "    batch_size = outputs.size()[0]\n",
        "    eps = 1e-5\n",
        "    if not per_image:\n",
        "        batch_size = 1\n",
        "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
        "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) + eps\n",
        "    loss = (1 - (2 * intersection + eps) / union).mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def jaccard(outputs, targets, per_image=False, non_empty=False, min_pixels=5):\n",
        "    batch_size = outputs.size()[0]\n",
        "    eps = 1e-3\n",
        "    if not per_image:\n",
        "        batch_size = 1\n",
        "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
        "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
        "    target_sum = torch.sum(dice_target, dim=1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    losses = 1 - (intersection + eps) / (torch.sum(dice_output + dice_target, dim=1) - intersection + eps)\n",
        "    if non_empty:\n",
        "        assert per_image == True\n",
        "        non_empty_images = 0\n",
        "        sum_loss = 0\n",
        "        for i in range(batch_size):\n",
        "            if target_sum[i] > min_pixels:\n",
        "                sum_loss += losses[i]\n",
        "                non_empty_images += 1\n",
        "        if non_empty_images == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return sum_loss / non_empty_images\n",
        "\n",
        "    return losses.mean()\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return soft_dice_loss(input, target, per_image=self.per_image)\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False, non_empty=False, apply_sigmoid=False,\n",
        "                 min_pixels=5):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "        self.non_empty = non_empty\n",
        "        self.apply_sigmoid = apply_sigmoid\n",
        "        self.min_pixels = min_pixels\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if self.apply_sigmoid:\n",
        "            input = torch.sigmoid(input)\n",
        "        return jaccard(input, target, per_image=self.per_image, non_empty=self.non_empty, min_pixels=self.min_pixels)\n",
        "\n",
        "\n",
        "class StableBCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StableBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input = input.float().contiguous().view(-1)\n",
        "        target = target.float().contiguous().view(-1)\n",
        "        neg_abs = - input.abs()\n",
        "        # todo check correctness\n",
        "        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
        "        return loss.mean()\n",
        "\n",
        "class FocalLoss2d(nn.Module):\n",
        "    def __init__(self, gamma=2, ignore_index=255):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        eps = 1e-8\n",
        "        non_ignored = targets.contiguous().view(-1) != self.ignore_index\n",
        "        targets = targets.contiguous().view(-1)[non_ignored].float()\n",
        "        outputs = outputs.contiguous().view(-1)[non_ignored]\n",
        "        outputs = torch.clamp(outputs, eps, 1. - eps)\n",
        "        targets = torch.clamp(targets, eps, 1. - eps)\n",
        "        pt = (1 - targets) * (1 - outputs) + targets * outputs\n",
        "        return (-(1. - pt) ** self.gamma * torch.log(pt)).mean()\n",
        "\n",
        "class ComboLoss(nn.Module):\n",
        "    def __init__(self, weights, per_image=False, channel_weights=[1, 0.5, 0.5], channel_losses=None):\n",
        "        super().__init__()\n",
        "        self.weights = weights\n",
        "        self.bce = StableBCELoss()\n",
        "        self.dice = DiceLoss(per_image=False)\n",
        "        self.jaccard = JaccardLoss(per_image=False)\n",
        "        self.focal = FocalLoss2d()\n",
        "        self.mapping = {'bce': self.bce,\n",
        "                        'dice': self.dice,\n",
        "                        'focal': self.focal,\n",
        "                        'jaccard': self.jaccard}\n",
        "        self.expect_sigmoid = {'dice', 'focal', 'jaccard'}\n",
        "        self.per_channel = {'dice', 'jaccard'}\n",
        "        self.values = {}\n",
        "        self.channel_weights = channel_weights\n",
        "        self.channel_losses = channel_losses\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = 0\n",
        "        weights = self.weights\n",
        "        sigmoid_input = torch.softmax(outputs, dim=1)\n",
        "        for k, v in weights.items():\n",
        "            if not v:\n",
        "                continue\n",
        "            val = 0 \n",
        "            if k in self.per_channel:\n",
        "                channels = targets.size(1)\n",
        "                for c in range(channels):\n",
        "                    if not self.channel_losses or k in self.channel_losses[c]:\n",
        "                        val += self.channel_weights[c] * self.mapping[k](sigmoid_input[:, c, ...] if k in self.expect_sigmoid else outputs[:, c, ...],\n",
        "                                               targets[:, c, ...])\n",
        "\n",
        "            else:\n",
        "                val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n",
        "            self.values[k] = val\n",
        "            loss += self.weights[k] * val\n",
        "            # print(k, val)\n",
        "        return loss.clamp(min=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik8gRntRq3Ux"
      },
      "source": [
        "## Over9000 Optimizer . Inspired by Iafoss . Over and Out !\n",
        "##https://github.com/mgrankin/over9000/blob/master/ralamb.py\n",
        "import torch, math\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "# RAdam + LARS\n",
        "class Ralamb(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-2, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(Ralamb, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Ralamb, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Ralamb does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # m_t\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                # v_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, radam_step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = radam_step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                radam_step = p_data_fp32.clone()\n",
        "                if N_sma >= 5:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n",
        "                else:\n",
        "                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n",
        "\n",
        "                radam_norm = radam_step.pow(2).sum().sqrt()\n",
        "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
        "                if weight_norm == 0 or radam_norm == 0:\n",
        "                    trust_ratio = 1\n",
        "                else:\n",
        "                    trust_ratio = weight_norm / radam_norm\n",
        "\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = radam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "\n",
        "                if N_sma >= 5:\n",
        "                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Lookahead implementation from https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/lookahead.py\n",
        "\n",
        "\"\"\" Lookahead Optimizer Wrapper.\n",
        "Implementation modified from: https://github.com/alphadl/lookahead.pytorch\n",
        "Paper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from collections import defaultdict\n",
        "\n",
        "class Lookahead(Optimizer):\n",
        "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
        "        self.base_optimizer = base_optimizer\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults = base_optimizer.defaults\n",
        "        self.defaults.update(defaults)\n",
        "        self.state = defaultdict(dict)\n",
        "        # manually add our defaults to the param groups\n",
        "        for name, default in defaults.items():\n",
        "            for group in self.param_groups:\n",
        "                group.setdefault(name, default)\n",
        "\n",
        "    def update_slow(self, group):\n",
        "        for fast_p in group[\"params\"]:\n",
        "            if fast_p.grad is None:\n",
        "                continue\n",
        "            param_state = self.state[fast_p]\n",
        "            if 'slow_buffer' not in param_state:\n",
        "                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
        "                param_state['slow_buffer'].copy_(fast_p.data)\n",
        "            slow = param_state['slow_buffer']\n",
        "            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
        "            fast_p.data.copy_(slow)\n",
        "\n",
        "    def sync_lookahead(self):\n",
        "        for group in self.param_groups:\n",
        "            self.update_slow(group)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        # print(self.k)\n",
        "        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n",
        "        loss = self.base_optimizer.step(closure)\n",
        "        for group in self.param_groups:\n",
        "            group['lookahead_step'] += 1\n",
        "            if group['lookahead_step'] % group['lookahead_k'] == 0:\n",
        "                self.update_slow(group)\n",
        "        return loss\n",
        "\n",
        "    def state_dict(self):\n",
        "        fast_state_dict = self.base_optimizer.state_dict()\n",
        "        slow_state = {\n",
        "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
        "            for k, v in self.state.items()\n",
        "        }\n",
        "        fast_state = fast_state_dict['state']\n",
        "        param_groups = fast_state_dict['param_groups']\n",
        "        return {\n",
        "            'state': fast_state,\n",
        "            'slow_state': slow_state,\n",
        "            'param_groups': param_groups,\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        fast_state_dict = {\n",
        "            'state': state_dict['state'],\n",
        "            'param_groups': state_dict['param_groups'],\n",
        "        }\n",
        "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
        "\n",
        "        # We want to restore the slow state, but share param_groups reference\n",
        "        # with base_optimizer. This is a bit redundant but least code\n",
        "        slow_state_new = False\n",
        "        if 'slow_state' not in state_dict:\n",
        "            print('Loading state_dict from optimizer without Lookahead applied.')\n",
        "            state_dict['slow_state'] = defaultdict(dict)\n",
        "            slow_state_new = True\n",
        "        slow_state_dict = {\n",
        "            'state': state_dict['slow_state'],\n",
        "            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
        "        }\n",
        "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
        "        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
        "        if slow_state_new:\n",
        "            # reapply defaults to catch missing lookahead specific ones\n",
        "            for name, default in self.defaults.items():\n",
        "                for group in self.param_groups:\n",
        "                    group.setdefault(name, default)\n",
        "\n",
        "def LookaheadAdam(params, alpha=0.5, k=6, *args, **kwargs):\n",
        "     adam = Adam(params, *args, **kwargs)\n",
        "     return Lookahead(adam, alpha, k)\n",
        "\n",
        "\n",
        "# RAdam + LARS + LookAHead\n",
        "\n",
        "# Lookahead implementation from https://github.com/lonePatient/lookahead_pytorch/blob/master/optimizer.py\n",
        "# RAdam + LARS implementation from https://gist.github.com/redknightlois/c4023d393eb8f92bb44b2ab582d7ec20\n",
        "\n",
        "def Over9000(params, alpha=0.5, k=6, *args, **kwargs):\n",
        "     ralamb = Ralamb(params, *args, **kwargs)\n",
        "     return Lookahead(ralamb, alpha, k)\n",
        "\n",
        "RangerLars = Over9000 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y34xlu5FB8-"
      },
      "source": [
        "# criterion = MixedLoss(10.0, 1.0)\n",
        "criterion = ComboLoss(weights={'bce': 5,'dice': 1,'focal': 5},\n",
        "                      # channel_weights=[0.01, 0.01 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
        "                      channel_weights=[0.0001, 0.1813306 , 0.01049447, 0.00313614, 0.04424467, 0.05193036, 0.04788384, 0.00945399, 0.02173117, 0.00382078, 0.46088194, 0.16509204],\n",
        "                      channel_losses=0)\n",
        "# optimizer = torch.optim.AdamW(params = model.parameters(), lr = 0.1, weight_decay=1e-6)\n",
        "optimizer = Over9000(model.parameters(), lr=1e-6, weight_decay=1e-4) \n",
        "# optimizer = RAdam(model.parameters(),lr = 1e-7, weight_decay=1e-4) \n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=1, eta_min=1e-8, last_epoch=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLcapv9SFDuE",
        "outputId": "d29e4cf6-d667-4a1a-f301-12654b8553c7"
      },
      "source": [
        "train(num_epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, saved_dir, val_every, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training..\n",
            "Epoch [1/15], Step [20/409], Loss: 0.2630, lr: 1e-06\n",
            "Epoch [1/15], Step [40/409], Loss: 0.2830, lr: 1e-06\n",
            "Epoch [1/15], Step [60/409], Loss: 0.4148, lr: 1e-06\n",
            "Epoch [1/15], Step [80/409], Loss: 0.1790, lr: 1e-06\n",
            "Epoch [1/15], Step [100/409], Loss: 0.1261, lr: 1e-06\n",
            "Epoch [1/15], Step [120/409], Loss: 0.1975, lr: 1e-06\n",
            "Epoch [1/15], Step [140/409], Loss: 0.1270, lr: 1e-06\n",
            "Epoch [1/15], Step [160/409], Loss: 0.3171, lr: 1e-06\n",
            "Epoch [1/15], Step [180/409], Loss: 0.1763, lr: 1e-06\n",
            "Epoch [1/15], Step [200/409], Loss: 0.2680, lr: 1e-06\n",
            "Epoch [1/15], Step [220/409], Loss: 0.4515, lr: 1e-06\n",
            "Epoch [1/15], Step [240/409], Loss: 0.1354, lr: 1e-06\n",
            "Epoch [1/15], Step [260/409], Loss: 0.2871, lr: 1e-06\n",
            "Epoch [1/15], Step [280/409], Loss: 0.1990, lr: 1e-06\n",
            "Epoch [1/15], Step [300/409], Loss: 0.2913, lr: 1e-06\n",
            "Epoch [1/15], Step [320/409], Loss: 0.3057, lr: 1e-06\n",
            "Epoch [1/15], Step [340/409], Loss: 0.2725, lr: 1e-06\n",
            "Epoch [1/15], Step [360/409], Loss: 0.2563, lr: 1e-06\n",
            "Epoch [1/15], Step [380/409], Loss: 0.3761, lr: 1e-06\n",
            "Epoch [1/15], Step [400/409], Loss: 0.2580, lr: 1e-06\n",
            "Epoch 1 - mIoU: 0.5223, acc: 0.8884\n",
            "Epoch [2/15], Step [20/409], Loss: 0.1448, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [40/409], Loss: 0.2063, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [60/409], Loss: 0.3225, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [80/409], Loss: 0.2185, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [100/409], Loss: 0.2249, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [120/409], Loss: 0.2385, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [140/409], Loss: 0.2936, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [160/409], Loss: 0.3112, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [180/409], Loss: 0.2728, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [200/409], Loss: 0.2743, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [220/409], Loss: 0.2962, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [240/409], Loss: 0.3826, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [260/409], Loss: 0.3012, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [280/409], Loss: 0.1873, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [300/409], Loss: 0.2065, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [320/409], Loss: 0.1693, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [340/409], Loss: 0.3054, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [360/409], Loss: 0.3818, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [380/409], Loss: 0.2195, lr: 9.891830623632337e-07\n",
            "Epoch [2/15], Step [400/409], Loss: 0.2378, lr: 9.891830623632337e-07\n",
            "Epoch 2 - mIoU: 0.5262, acc: 0.8905\n",
            "Epoch [3/15], Step [20/409], Loss: 0.2957, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [40/409], Loss: 0.1461, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [60/409], Loss: 0.1641, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [80/409], Loss: 0.3956, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [100/409], Loss: 0.1454, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [120/409], Loss: 0.3664, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [140/409], Loss: 0.2452, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [160/409], Loss: 0.3153, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [180/409], Loss: 0.1641, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [200/409], Loss: 0.1423, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [220/409], Loss: 0.1575, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [240/409], Loss: 0.1730, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [260/409], Loss: 0.1456, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [280/409], Loss: 0.2454, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [300/409], Loss: 0.2683, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [320/409], Loss: 0.2271, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [340/409], Loss: 0.2712, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [360/409], Loss: 0.2961, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [380/409], Loss: 0.2465, lr: 9.572050015330873e-07\n",
            "Epoch [3/15], Step [400/409], Loss: 0.1683, lr: 9.572050015330873e-07\n",
            "Epoch 3 - mIoU: 0.5321, acc: 0.8925\n",
            "Epoch [4/15], Step [20/409], Loss: 0.3350, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [40/409], Loss: 0.2673, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [60/409], Loss: 0.2500, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [80/409], Loss: 0.3920, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [100/409], Loss: 0.2322, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [120/409], Loss: 0.1998, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [140/409], Loss: 0.2316, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [160/409], Loss: 0.1802, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [180/409], Loss: 0.2896, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [200/409], Loss: 0.2083, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [220/409], Loss: 0.1587, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [240/409], Loss: 0.2239, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [260/409], Loss: 0.2307, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [280/409], Loss: 0.2132, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [300/409], Loss: 0.1628, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [320/409], Loss: 0.2776, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [340/409], Loss: 0.1213, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [360/409], Loss: 0.2001, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [380/409], Loss: 0.2281, lr: 9.05463412215599e-07\n",
            "Epoch [4/15], Step [400/409], Loss: 0.2609, lr: 9.05463412215599e-07\n",
            "Epoch 4 - mIoU: 0.5278, acc: 0.8876\n",
            "Epoch [5/15], Step [20/409], Loss: 0.1741, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [40/409], Loss: 0.1720, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [60/409], Loss: 0.1579, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [80/409], Loss: 0.1280, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [100/409], Loss: 0.1739, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [120/409], Loss: 0.2188, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [140/409], Loss: 0.2880, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [160/409], Loss: 0.3251, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [180/409], Loss: 0.1656, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [200/409], Loss: 0.2681, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [220/409], Loss: 0.1629, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [240/409], Loss: 0.1896, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [260/409], Loss: 0.4237, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [280/409], Loss: 0.1148, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [300/409], Loss: 0.2455, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [320/409], Loss: 0.2422, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [340/409], Loss: 0.2012, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [360/409], Loss: 0.3366, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [380/409], Loss: 0.2324, lr: 8.362196501476348e-07\n",
            "Epoch [5/15], Step [400/409], Loss: 0.1807, lr: 8.362196501476348e-07\n",
            "Epoch 5 - mIoU: 0.5283, acc: 0.8928\n",
            "Epoch [6/15], Step [20/409], Loss: 0.2898, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [40/409], Loss: 0.1412, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [60/409], Loss: 0.1668, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [80/409], Loss: 0.1862, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [100/409], Loss: 0.1630, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [120/409], Loss: 0.2228, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [140/409], Loss: 0.2367, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [160/409], Loss: 0.2290, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [180/409], Loss: 0.1630, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [200/409], Loss: 0.1123, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [220/409], Loss: 0.2819, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [240/409], Loss: 0.1782, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [260/409], Loss: 0.2908, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [280/409], Loss: 0.1656, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [300/409], Loss: 0.2879, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [320/409], Loss: 0.1352, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [340/409], Loss: 0.1259, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [360/409], Loss: 0.1529, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [380/409], Loss: 0.1849, lr: 7.525000000000001e-07\n",
            "Epoch [6/15], Step [400/409], Loss: 0.1926, lr: 7.525000000000001e-07\n",
            "Epoch 6 - mIoU: 0.5285, acc: 0.8943\n",
            "Epoch [7/15], Step [20/409], Loss: 0.1809, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [40/409], Loss: 0.1846, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [60/409], Loss: 0.1818, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [80/409], Loss: 0.1696, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [100/409], Loss: 0.2185, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [120/409], Loss: 0.2540, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [140/409], Loss: 0.1581, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [160/409], Loss: 0.2470, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [180/409], Loss: 0.1821, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [200/409], Loss: 0.1609, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [220/409], Loss: 0.1930, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [240/409], Loss: 0.1831, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [260/409], Loss: 0.1924, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [280/409], Loss: 0.1769, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [300/409], Loss: 0.3200, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [320/409], Loss: 0.2468, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [340/409], Loss: 0.1762, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [360/409], Loss: 0.1703, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [380/409], Loss: 0.3003, lr: 6.57963412215599e-07\n",
            "Epoch [7/15], Step [400/409], Loss: 0.2051, lr: 6.57963412215599e-07\n",
            "Epoch 7 - mIoU: 0.5320, acc: 0.8935\n",
            "Epoch [8/15], Step [20/409], Loss: 0.1937, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [40/409], Loss: 0.2860, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [60/409], Loss: 0.1283, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [80/409], Loss: 0.3301, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [100/409], Loss: 0.1229, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [120/409], Loss: 0.2627, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [140/409], Loss: 0.2168, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [160/409], Loss: 0.1599, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [180/409], Loss: 0.2729, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [200/409], Loss: 0.1705, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [220/409], Loss: 0.1861, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [240/409], Loss: 0.1356, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [260/409], Loss: 0.1991, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [280/409], Loss: 0.1450, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [300/409], Loss: 0.1844, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [320/409], Loss: 0.1614, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [340/409], Loss: 0.1470, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [360/409], Loss: 0.1639, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [380/409], Loss: 0.1750, lr: 5.567415893174886e-07\n",
            "Epoch [8/15], Step [400/409], Loss: 0.1826, lr: 5.567415893174886e-07\n",
            "Epoch 8 - mIoU: 0.5329, acc: 0.8932\n",
            "Epoch [9/15], Step [20/409], Loss: 0.1923, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [40/409], Loss: 0.3066, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [60/409], Loss: 0.1910, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [80/409], Loss: 0.2341, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [100/409], Loss: 0.2440, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [120/409], Loss: 0.1666, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [140/409], Loss: 0.2067, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [160/409], Loss: 0.1746, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [180/409], Loss: 0.2002, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [200/409], Loss: 0.1786, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [220/409], Loss: 0.1899, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [240/409], Loss: 0.2199, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [260/409], Loss: 0.3311, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [280/409], Loss: 0.2787, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [300/409], Loss: 0.2861, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [320/409], Loss: 0.3105, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [340/409], Loss: 0.1669, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [360/409], Loss: 0.2068, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [380/409], Loss: 0.2580, lr: 4.5325841068251165e-07\n",
            "Epoch [9/15], Step [400/409], Loss: 0.3313, lr: 4.5325841068251165e-07\n",
            "Epoch 9 - mIoU: 0.5308, acc: 0.8926\n",
            "Epoch [10/15], Step [20/409], Loss: 0.2141, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [40/409], Loss: 0.1947, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [60/409], Loss: 0.1449, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [80/409], Loss: 0.2530, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [100/409], Loss: 0.1503, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [120/409], Loss: 0.3313, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [140/409], Loss: 0.3560, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [160/409], Loss: 0.1917, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [180/409], Loss: 0.2629, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [200/409], Loss: 0.3046, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [220/409], Loss: 0.2106, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [240/409], Loss: 0.1807, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [260/409], Loss: 0.2200, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [280/409], Loss: 0.2746, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [300/409], Loss: 0.2487, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [320/409], Loss: 0.1438, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [340/409], Loss: 0.3162, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [360/409], Loss: 0.2398, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [380/409], Loss: 0.2783, lr: 3.520365877844011e-07\n",
            "Epoch [10/15], Step [400/409], Loss: 0.3358, lr: 3.520365877844011e-07\n",
            "Epoch 10 - mIoU: 0.5273, acc: 0.8955\n",
            "Epoch [11/15], Step [20/409], Loss: 0.2108, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [40/409], Loss: 0.1621, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [60/409], Loss: 0.2430, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [80/409], Loss: 0.3158, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [100/409], Loss: 0.1614, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [120/409], Loss: 0.1689, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [140/409], Loss: 0.2019, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [160/409], Loss: 0.2940, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [180/409], Loss: 0.2648, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [200/409], Loss: 0.3885, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [220/409], Loss: 0.2000, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [240/409], Loss: 0.2069, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [260/409], Loss: 0.2666, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [280/409], Loss: 0.2315, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [300/409], Loss: 0.2619, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [320/409], Loss: 0.2391, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [340/409], Loss: 0.2478, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [360/409], Loss: 0.2156, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [380/409], Loss: 0.1917, lr: 2.5750000000000013e-07\n",
            "Epoch [11/15], Step [400/409], Loss: 0.1595, lr: 2.5750000000000013e-07\n",
            "Epoch 11 - mIoU: 0.5274, acc: 0.8928\n",
            "Epoch [12/15], Step [20/409], Loss: 0.1848, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [40/409], Loss: 0.2220, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [60/409], Loss: 0.1911, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [80/409], Loss: 0.3168, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [100/409], Loss: 0.1298, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [120/409], Loss: 0.1972, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [140/409], Loss: 0.1877, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [160/409], Loss: 0.1549, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [180/409], Loss: 0.1601, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [200/409], Loss: 0.3501, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [220/409], Loss: 0.2156, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [240/409], Loss: 0.2413, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [260/409], Loss: 0.1892, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [280/409], Loss: 0.1932, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [300/409], Loss: 0.2071, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [320/409], Loss: 0.1744, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [340/409], Loss: 0.1387, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [360/409], Loss: 0.2402, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [380/409], Loss: 0.1183, lr: 1.7378034985236534e-07\n",
            "Epoch [12/15], Step [400/409], Loss: 0.2752, lr: 1.7378034985236534e-07\n",
            "Epoch 12 - mIoU: 0.5321, acc: 0.8933\n",
            "Epoch [13/15], Step [20/409], Loss: 0.2032, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [40/409], Loss: 0.1961, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [60/409], Loss: 0.2668, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [80/409], Loss: 0.2382, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [100/409], Loss: 0.2488, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [120/409], Loss: 0.1926, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [140/409], Loss: 0.2184, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [160/409], Loss: 0.1851, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [180/409], Loss: 0.2330, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [200/409], Loss: 0.1568, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [220/409], Loss: 0.1928, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [240/409], Loss: 0.2216, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [260/409], Loss: 0.2297, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [280/409], Loss: 0.1300, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [300/409], Loss: 0.1461, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [320/409], Loss: 0.1629, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [340/409], Loss: 0.1263, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [360/409], Loss: 0.3284, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [380/409], Loss: 0.3861, lr: 1.0453658778440107e-07\n",
            "Epoch [13/15], Step [400/409], Loss: 0.2278, lr: 1.0453658778440107e-07\n",
            "Epoch 13 - mIoU: 0.5272, acc: 0.8909\n",
            "Epoch [14/15], Step [20/409], Loss: 0.2667, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [40/409], Loss: 0.2101, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [60/409], Loss: 0.1209, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [80/409], Loss: 0.3075, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [100/409], Loss: 0.2675, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [120/409], Loss: 0.3683, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [140/409], Loss: 0.2006, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [160/409], Loss: 0.2552, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [180/409], Loss: 0.1262, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [200/409], Loss: 0.2200, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [220/409], Loss: 0.1841, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [240/409], Loss: 0.1703, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [260/409], Loss: 0.1886, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [280/409], Loss: 0.1522, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [300/409], Loss: 0.1648, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [320/409], Loss: 0.2162, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [340/409], Loss: 0.3174, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [360/409], Loss: 0.2536, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [380/409], Loss: 0.3213, lr: 5.279499846691252e-08\n",
            "Epoch [14/15], Step [400/409], Loss: 0.2685, lr: 5.279499846691252e-08\n",
            "Epoch 14 - mIoU: 0.5257, acc: 0.8914\n",
            "Epoch [15/15], Step [20/409], Loss: 0.2150, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [40/409], Loss: 0.1518, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [60/409], Loss: 0.2661, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [80/409], Loss: 0.2758, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [100/409], Loss: 0.3259, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [120/409], Loss: 0.2094, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [140/409], Loss: 0.1772, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [160/409], Loss: 0.2387, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [180/409], Loss: 0.1850, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [200/409], Loss: 0.2297, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [220/409], Loss: 0.2613, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [240/409], Loss: 0.2381, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [260/409], Loss: 0.3142, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [280/409], Loss: 0.1616, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [300/409], Loss: 0.1650, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [320/409], Loss: 0.1777, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [340/409], Loss: 0.1915, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [360/409], Loss: 0.2626, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [380/409], Loss: 0.2955, lr: 2.0816937636766187e-08\n",
            "Epoch [15/15], Step [400/409], Loss: 0.2785, lr: 2.0816937636766187e-08\n",
            "Epoch 15 - mIoU: 0.5284, acc: 0.8962\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}