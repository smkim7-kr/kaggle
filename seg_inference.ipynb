{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference2_condpseudo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f49nU0EHLzZ_"
      },
      "source": [
        "# !pip install -q pytorch_toolbelt\n",
        "# !pip install -q torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0\n",
        "# !pip install -q git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "# !pip install -q pycocotools\n",
        "# !pip install -q cython\n",
        "# !pip install -q git+https://github.com/lucasb-eyer/pydensecrf.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K5G7Mcd6Y1G"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGNguwP6L2ob",
        "outputId": "5c6cda8b-d7ca-4625-bf4e-72f35d5e8f28"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Python package for pre-processing \n",
        "from pycocotools.coco import COCO\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_toolbelt import losses as L\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensor\n",
        "\n",
        "# Python package for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "plt.rcParams['axes.grid'] = False\n",
        "\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import unary_from_labels, create_pairwise_bilateral\n",
        "from skimage.color import gray2rgb\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "print('Pytorch version: {}'.format(torch.__version__))\n",
        "print('Is GPU available: {}'.format(torch.cuda.is_available()))\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "  print('The number of GPUs available: {}'.format(torch.cuda.device_count()))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "\n",
        "print('CPU count: {}'.format(os.cpu_count()))  # 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version: 1.8.0\n",
            "Is GPU available: True\n",
            "Tesla V100-SXM2-16GB\n",
            "The number of GPUs available: 1\n",
            "CPU count: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDuH52N6xgM4"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/segment/data'\n",
        "anns_file_path = os.path.join(dataset_path, 'test.json')\n",
        "\n",
        "# Read annotations\n",
        "with open(anns_file_path, 'r') as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "categories = dataset['categories']\n",
        "anns = dataset['annotations']\n",
        "imgs = dataset['images']\n",
        "nr_cats = len(categories)\n",
        "nr_annotations = len(anns)\n",
        "nr_images = len(imgs)\n",
        "\n",
        "# Load categories and super categories\n",
        "cat_names = []\n",
        "super_cat_names = []\n",
        "super_cat_ids = {}\n",
        "super_cat_last_name = ''\n",
        "nr_super_cats = 0\n",
        "for cat_it in categories:\n",
        "    cat_names.append(cat_it['name'])\n",
        "    super_cat_name = cat_it['supercategory']\n",
        "    # Adding new supercat\n",
        "    if super_cat_name != super_cat_last_name:\n",
        "        super_cat_names.append(super_cat_name)\n",
        "        super_cat_ids[super_cat_name] = nr_super_cats\n",
        "        super_cat_last_name = super_cat_name\n",
        "        nr_super_cats += 1\n",
        "\n",
        "print('Number of super categories:', nr_super_cats)\n",
        "print('Number of categories:', nr_cats)\n",
        "print('Number of annotations:', nr_annotations)\n",
        "print('Number of images:', nr_images)\n",
        "\n",
        "# Count annotations\n",
        "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
        "for ann in anns:\n",
        "    cat_histogram[ann['category_id']] += 1\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
        "df = df.sort_values('Number of annotations', 0, False)\n",
        "\n",
        "sorted_temp_df = df.sort_index()\n",
        "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
        "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)\n",
        "sorted_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFGh044y7721"
      },
      "source": [
        "category_names = list(sorted_df.Categories)\n",
        "def get_classname(classID, cats):\n",
        "    for i in range(len(cats)):\n",
        "        if cats[i]['id']==classID:\n",
        "            return cats[i]['name']\n",
        "    return \"None\"\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"COCO format\"\"\"\n",
        "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.coco = COCO(data_dir)\n",
        "        \n",
        "    def __getitem__(self, index: int):\n",
        "        # Get the image_info using coco library\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "        image_infos = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # Load the image using opencv\n",
        "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
        "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        images /= 255.0\n",
        "        \n",
        "        if (self.mode in ('train', 'val')):\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
        "            anns = self.coco.loadAnns(ann_ids)\n",
        "            # print(\"image_infos['id'] : {}\".format(image_infos['id']) )\n",
        "            # Load the categories in a variable\n",
        "            cat_ids = self.coco.getCatIds()\n",
        "            cats = self.coco.loadCats(cat_ids)\n",
        "\n",
        "            # masks_size : height x width            \n",
        "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]), dtype=np.float32)\n",
        "  \n",
        "            # Background = 0, Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
        "            for i in range(len(anns)):\n",
        "                className = get_classname(anns[i]['category_id'], cats)\n",
        "                pixel_value = category_names.index(className)\n",
        "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
        "            \n",
        "\n",
        "            # We can use Albumentations for image & mask transformation(or augmentation)\n",
        "            if self.transform is not None:\n",
        "                transformed = self.transform(image=images, mask=masks)\n",
        "                images = transformed[\"image\"]\n",
        "                masks = transformed[\"mask\"]\n",
        "                masks =  masks.squeeze()\n",
        "            \n",
        "            return images, masks, image_infos\n",
        "        \n",
        "        if self.mode == 'test':            \n",
        "            if self.transform is not None:\n",
        "                transformed = self.transform(image=images)\n",
        "                images = transformed[\"image\"]\n",
        "            \n",
        "            return images, image_infos\n",
        "    \n",
        "    \n",
        "    def __len__(self) -> int:        \n",
        "        return len(self.coco.getImgIds())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtFkvsdbxtNE",
        "outputId": "b62b8ef3-ae52-4e96-8721-b0880cd9e0f1"
      },
      "source": [
        "test_path = os.path.join(dataset_path, 'test.json')\n",
        "\n",
        "# collate_fn needs for batch\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "test_transform = A.Compose([\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "test_tta_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=1),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "test_vtta_transform = A.Compose([\n",
        "    A.VerticalFlip(p=1),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "# test dataset\n",
        "test_dataset = CustomDataset(data_dir=test_path, mode='test', transform=test_transform)\n",
        "test_tta_dataset = CustomDataset(data_dir=test_path, mode='test', transform=test_tta_transform)\n",
        "test_vtta_dataset = CustomDataset(data_dir=test_path, mode='test', transform=test_vtta_transform)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=3,\n",
        "                                          pin_memory=True,\n",
        "                                          num_workers=4,\n",
        "                                          collate_fn=collate_fn)\n",
        "\n",
        "test_tta_loader = torch.utils.data.DataLoader(dataset=test_tta_dataset,\n",
        "                                          batch_size=3,\n",
        "                                          pin_memory=True,\n",
        "                                          num_workers=4,\n",
        "                                          collate_fn=collate_fn)\n",
        "\n",
        "test_vtta_loader = torch.utils.data.DataLoader(dataset=test_vtta_dataset,\n",
        "                                          batch_size=3,\n",
        "                                          pin_memory=True,\n",
        "                                          num_workers=4,\n",
        "                                          collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZdqYVdwi42a"
      },
      "source": [
        "models = list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLv22aB4Kzod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52547e2-0db3-4c32-91e8-0b1513a32e8c"
      },
      "source": [
        "model_path = '/content/drive/MyDrive/segment/saved/realrealfinal/finalep_best_model_eff3.pt'\n",
        "# model_path = '/content/drive/MyDrive/segment/saved/realfinal/40ep_best_model_resnext50_320.pt'\n",
        "\n",
        "# initialize the model\n",
        "models.append(smp.DeepLabV3Plus(encoder_name='tu-efficientnet_b3', classes=12, encoder_weights='imagenet').to(device))\n",
        "# load the saved best model\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "state_dict = checkpoint.state_dict()\n",
        "models[0].load_state_dict(state_dict)\n",
        "\n",
        "# switch to evaluation mode\n",
        "models[0].eval()\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMmQLrkUhtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5f2128-46ee-4e81-890c-10fc26d12662"
      },
      "source": [
        "model_path = '/content/drive/MyDrive/segment/saved/realrealfinal/finalep_best_model_seresnext50.pt'\n",
        "# model_path = '/content/drive/MyDrive/segment/saved/realfinal/40ep_best_model_resnext50_320.pt'\n",
        "\n",
        "# initialize the model\n",
        "models.append(smp.DeepLabV3Plus(encoder_name='tu-seresnext50_32x4d', classes=12, encoder_weights='imagenet').to(device))\n",
        "# load the saved best model\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "state_dict = checkpoint.state_dict()\n",
        "models[1].load_state_dict(state_dict)\n",
        "\n",
        "# switch to evaluation mode\n",
        "models[1].eval()\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9GzNWECiAj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c900e9-011f-4890-b203-d4a28378fbd7"
      },
      "source": [
        "# path of saved best model\n",
        "model_path = '/content/drive/MyDrive/segment/saved/realrealfinal/finalep_best_model_eff3_dsize.pt'\n",
        "# model_path = '/content/drive/MyDrive/segment/saved/realfinal/40ep_best_model_eff3_480.pt'\n",
        "\n",
        "# initialize the model\n",
        "models.append(smp.DeepLabV3Plus(encoder_name='tu-seresnext50_32x4d', classes=12, encoder_weights='imagenet').to(device))\n",
        "# load the saved best model\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "state_dict = checkpoint.state_dict()\n",
        "models[2].load_state_dict(state_dict)\n",
        "\n",
        "# switch to evaluation mode\n",
        "models[2].eval()\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecIMtkCFOMRq",
        "outputId": "292c02fd-bbfe-4f6a-faad-79edf3d38a4a"
      },
      "source": [
        "model_path = '/content/drive/MyDrive/segment/saved/realrealfinal/finalep_best_model_eff3_tmp.pt'\n",
        "# model_path = '/content/drive/MyDrive/segment/saved/realfinal/40ep_best_model_resnext50_480.pt'\n",
        "\n",
        "# initialize the model\n",
        "models.append(smp.DeepLabV3Plus(encoder_name='tu-efficientnet_b3', classes=12, encoder_weights='imagenet').to(device))\n",
        "# load the saved best model\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "state_dict = checkpoint.state_dict()\n",
        "models[3].load_state_dict(state_dict)\n",
        "\n",
        "# switch to evaluation mode\n",
        "models[3].eval()\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcUVRUOwZhCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9996522-04f4-4ccc-ec87-6a6d0e1547e4"
      },
      "source": [
        "# path of saved best model\n",
        "model_path = '/content/drive/MyDrive/segment/saved/realrealfinal/finalep_best_model_eff3_sampler_320.pt'\n",
        "# model_path = '/content/drive/MyDrive/segment/saved/realfinal/40ep_best_model_eff3_320.pt'\n",
        "\n",
        "# initialize the model\n",
        "models.append(smp.DeepLabV3Plus(encoder_name='tu-efficientnet_b3', classes=12, encoder_weights='imagenet').to(device))\n",
        "# load the saved best model\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "state_dict = checkpoint.state_dict()\n",
        "models[4].load_state_dict(state_dict)\n",
        "\n",
        "# switch to evaluation mode\n",
        "models[4].eval()\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isJ5EwSWpaB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3abe3cf-1a07-408f-f0b6-9e0f1f61cb16"
      },
      "source": [
        "# model_path = '/content/drive/MyDrive/segment/saved/realfinal/finalep_best_model_wresnet50_320.pt'\n",
        "model_path = '/content/drive/MyDrive/segment/saved/realrealfinal/finalep_best_model_eff3_sampler.pt'\n",
        "\n",
        "# initialize the model\n",
        "models.append(smp.DeepLabV3Plus(encoder_name='tu-efficientnet_b3', classes=12, encoder_weights='imagenet').to(device))\n",
        "# load the saved best model\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "state_dict = checkpoint.state_dict()\n",
        "models[5].load_state_dict(state_dict)\n",
        "\n",
        "# switch to evaluation mode\n",
        "models[5].eval()\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izMvgo0-5Gop"
      },
      "source": [
        "for imgs, image_infos in test_tta_loader:\n",
        "    image_infos = image_infos\n",
        "    temp_images = imgs\n",
        "\n",
        "    models[0].eval()\n",
        "    # inference\n",
        "    outs = models[0](torch.stack(temp_images).to(device))\n",
        "    oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "    \n",
        "    break\n",
        "\n",
        "i = 0\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 16))\n",
        "\n",
        "print('Shape of Original Image :', list(temp_images[i].shape))\n",
        "print('Shape of Predicted : ', list(oms[i].shape))\n",
        "print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(oms[i]))])\n",
        "\n",
        "# Original image\n",
        "ax1.imshow(temp_images[i].permute([1,2,0]))\n",
        "ax1.grid(False)\n",
        "ax1.set_title(\"Original image : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
        "\n",
        "# Predicted mask\n",
        "ax2.imshow(oms[i])\n",
        "ax2.grid(False)\n",
        "ax2.set_title(\"Predicted : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejSVyCsPNMht"
      },
      "source": [
        "\"\"\"\n",
        "Function which returns the labelled image after applying CRF\n",
        "\n",
        "\"\"\"\n",
        "#Original_image = Image which has to labelled\n",
        "#Mask image = Which has been labelled by some technique..\n",
        "def crf(original_image, mask_img, labelmap):\n",
        "\n",
        "    # Converting annotated image to RGB if it is Gray scale\n",
        "    if(len(mask_img.shape)<3):\n",
        "        mask_img = gray2rgb(mask_img)\n",
        "\n",
        "#     #Converting the annotations RGB color to single 32 bit integer\n",
        "    annotated_label = mask_img[:,:,0] + (mask_img[:,:,1]<<8) + (mask_img[:,:,2]<<16)\n",
        "\n",
        "#     # Convert the 32bit integer color to 0,1, 2, ... labels.\n",
        "    colors, labels = np.unique(annotated_label, return_inverse=True)\n",
        "    \n",
        "    n_labels = len(labelmap)\n",
        "    #Setting up the CRF model\n",
        "    d = dcrf.DenseCRF2D(original_image.shape[1], original_image.shape[2], n_labels)\n",
        "\n",
        "    # get unary potentials (neg log probability)\n",
        "    U = unary_from_labels(labels, n_labels, gt_prob=0.7, zero_unsure=False)\n",
        "    d.setUnaryEnergy(U)\n",
        "\n",
        "    # This adds the color-independent term, features are the locations only.\n",
        "    d.addPairwiseGaussian(sxy=(3, 3), compat=3, kernel=dcrf.DIAG_KERNEL,\n",
        "                      normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "        \n",
        "    #Run Inference for 10 steps \n",
        "    Q = d.inference(10)\n",
        "\n",
        "    # Find out the most probable class for each pixel.\n",
        "    MAP = np.argmax(Q, axis=0)\n",
        "    \n",
        "    for i in range(n_labels):\n",
        "      MAP[MAP==i] = int(labelmap[i])\n",
        "    return MAP.reshape((original_image.shape[1],original_image.shape[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NwOSypYaRg7"
      },
      "source": [
        "def sharpen(p,t=0.5):\n",
        "        if t!=0:\n",
        "            return p**t\n",
        "        else:\n",
        "            return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LA1YeZGLiOL"
      },
      "source": [
        "t = 0\n",
        "for imgs, image_infos in test_loader:\n",
        "    t += 1\n",
        "    image_infos = image_infos\n",
        "    temp_images = imgs\n",
        "    outs = torch.zeros((3, 12, 512, 512)).to(device)\n",
        "    logits = torch.zeros((3, 12, 512, 512)).to(device)\n",
        "    crf_output = list()\n",
        "    for model in [models[1]]:\n",
        "      model.eval()\n",
        "      # inference\n",
        "      logits = model(torch.stack(temp_images).to(device))\n",
        "      outs += sharpen(torch.softmax(logits, dim=1))\n",
        "      # outs += sharpen(model(torch.stack(temp_images).to(device)))\n",
        "    outs /= len(models)\n",
        "    condpseudo = torch.amax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "    # print(condpseudo.mean(axis=(1,2)))\n",
        "    oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "    # print((condpseudo>0.9).sum(axis=(1,2)) / (320*320))\n",
        "    for i in range(3):\n",
        "      crf_output.append(crf(temp_images[i],oms[i], list(np.unique(oms[i]))))\n",
        "    np.array(crf_output).reshape((3, 512, 512))\n",
        "    if t==13:\n",
        "      break\n",
        "\n",
        "i = 2\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(16, 16))\n",
        "\n",
        "print('Shape of Original Image :', list(temp_images[i].shape))\n",
        "print('Shape of Predicted : ', list(oms[i].shape))\n",
        "print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(oms[i]))])\n",
        "print('Unique crf values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(crf_output[i]))])\n",
        "# Original image\n",
        "ax1.imshow(temp_images[i].permute([1,2,0]))\n",
        "ax1.grid(False)\n",
        "ax1.set_title(\"Original image : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
        "\n",
        "# Predicted mask\n",
        "ax2.imshow(oms[i])\n",
        "ax2.grid(False)\n",
        "ax2.set_title(\"Predicted : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
        "\n",
        "# CRF mask\n",
        "ax3.imshow(crf_output[i])\n",
        "ax3.grid(False)\n",
        "ax3.set_title(\"Predicted crf : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDsfUnJILkl1"
      },
      "source": [
        "def test(model, data_loader, device):\n",
        "    size = 256\n",
        "    transform = A.Compose([A.Resize(256, 256)])\n",
        "    print('Start prediction.')\n",
        "    for model in models:\n",
        "      model.eval()\n",
        "    \n",
        "    file_name_list = []\n",
        "    confidence_list = []\n",
        "    over90_list = []\n",
        "    over80_list = []\n",
        "    over70_list = []\n",
        "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for step, ((imgs, image_infos), (himgs, himage_infos), (vimgs, vimage_infos)) in enumerate(zip(test_loader, test_tta_loader, test_vtta_loader)):\n",
        "            preds = torch.zeros((3, 12, 512, 512)).to(device)\n",
        "            # inference (320 x 320)\n",
        "            for model in models:\n",
        "              outs = model(torch.stack(imgs).to(device))\n",
        "              houts = model(torch.stack(himgs).to(device))\n",
        "              vouts = model(torch.stack(vimgs).to(device))\n",
        "              ttaouts = torch.from_numpy(houts.detach().cpu().numpy()[:, :, :, ::-1].copy()).to(device)\n",
        "              vttaouts = torch.from_numpy(vouts.detach().cpu().numpy()[:, :, ::-1, :].copy()).to(device)\n",
        "              preds += (sharpen(torch.softmax(outs, dim=1))+sharpen(torch.softmax(ttaouts, dim=1))+sharpen(torch.softmax(vttaouts, dim=1)))/3\n",
        "            preds /= len(models)\n",
        "            oms = torch.argmax(preds.squeeze(), dim=1).detach().cpu().numpy()\n",
        "            condpseudo = torch.amax(preds.squeeze(), dim=1).detach().cpu().numpy()\n",
        "            confidence = condpseudo.mean(axis=(1,2))\n",
        "            over90 = (condpseudo>0.9).sum(axis=(1,2)) / (512*512)\n",
        "            over80 = (condpseudo>0.8).sum(axis=(1,2)) / (512*512)\n",
        "            over70 = (condpseudo>0.7).sum(axis=(1,2)) / (512*512)\n",
        "\n",
        "            confidence_list.extend(confidence)\n",
        "            over90_list.extend(over90)\n",
        "            over80_list.extend(over80)\n",
        "            over70_list.extend(over70)\n",
        "\n",
        "\n",
        "            # resize (256 x 256)\n",
        "            temp_mask = []\n",
        "            for img, mask in zip(np.stack(temp_images), oms):\n",
        "                transformed = transform(image=img, mask=mask)\n",
        "                mask = transformed['mask']\n",
        "                temp_mask.append(mask)\n",
        "\n",
        "            oms = np.array(temp_mask)\n",
        "            \n",
        "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
        "            preds_array = np.vstack((preds_array, oms))\n",
        "            \n",
        "            file_name_list.append([i['file_name'] for i in image_infos])\n",
        "    print(\"End prediction.\")\n",
        "    file_names = [y for x in file_name_list for y in x]\n",
        "    \n",
        "    return file_names, preds_array, confidence_list, over90_list, over80_list, over70_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0N8q-1aLob7",
        "outputId": "c99e2538-c417-447a-a3b3-9c0ed0210bcb"
      },
      "source": [
        "# inference\n",
        "file_names, preds, conf, o90, o80, o70 = test(models, test_loader, device)\n",
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['image_id'] = file_names\n",
        "submission['PredictionString'] = [' '.join(str(e) for e in string.tolist()) for string in preds]\n",
        "\n",
        "# save submission.csv\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start prediction.\n",
            "End prediction.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ugcOWQ0f2xu"
      },
      "source": [
        "submission['conf'] = conf\n",
        "submission['o90'] = o90\n",
        "submission['o80'] = o80\n",
        "submission['o70'] = o70\n",
        "submission.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37m6cHVTirQM"
      },
      "source": [
        "condidx = submission[(submission['conf']>0.955) & (submission['o90']>0.85)].index\n",
        "preds = preds[condidx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdykSUU3hO0j"
      },
      "source": [
        "category_names = list(sorted_df.Categories)\n",
        "def get_classname(classID, cats):\n",
        "    for i in range(len(cats)):\n",
        "        if cats[i]['id']==classID:\n",
        "            return cats[i]['name']\n",
        "    return \"None\"\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"COCO format\"\"\"\n",
        "    def __init__(self, data_dir, mode = 'train', transform = None, preds = preds):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.coco = COCO(data_dir)\n",
        "        self.preds = preds\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        # Get the image_info using coco library\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "        image_infos = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # Load the image using opencv\n",
        "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
        "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        images /= 255.0\n",
        "        \n",
        "        if (self.mode in ('train', 'val')):\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
        "            anns = self.coco.loadAnns(ann_ids)\n",
        "            # print(\"image_infos['id'] : {}\".format(image_infos['id']) )\n",
        "            # Load the categories in a variable\n",
        "            cat_ids = self.coco.getCatIds()\n",
        "            cats = self.coco.loadCats(cat_ids)\n",
        "\n",
        "            # masks_size : height x width            \n",
        "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]), dtype=np.float32)\n",
        "  \n",
        "            # Background = 0, Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
        "            for i in range(len(anns)):\n",
        "                className = get_classname(anns[i]['category_id'], cats)\n",
        "                pixel_value = category_names.index(className)\n",
        "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
        "\n",
        "            # We can use Albumentations for image & mask transformation(or augmentation)\n",
        "            if self.transform is not None:\n",
        "                transformed = self.transform(image=images, mask=masks)\n",
        "                images = transformed[\"image\"]\n",
        "                masks = transformed[\"mask\"]\n",
        "                masks =  masks.squeeze()\n",
        "            \n",
        "            return images, masks, image_infos\n",
        "        \n",
        "        if self.mode == 'test':\n",
        "            if self.transform is not None:\n",
        "                masks = np.array(preds[index])\n",
        "                masks = masks.reshape((256, 256))\n",
        "                transformed = self.transform(image=images, mask=masks)\n",
        "                images = transformed[\"image\"]\n",
        "                masks = transformed[\"mask\"]\n",
        "                masks =  masks.squeeze()\n",
        "\n",
        "            return images, masks, image_infos\n",
        "    \n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "      if self.mode == 'train':\n",
        "        return len(self.coco.getImgIds())\n",
        "      elif self.mode == 'test':\n",
        "        return len(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j76D91YFpSrM"
      },
      "source": [
        "# COCO(test_path).getImgIds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05WNO1t-he85",
        "outputId": "08b11d47-d712-4d86-da4a-35b6cd96bc30"
      },
      "source": [
        "test_path = os.path.join(dataset_path, 'test.json')\n",
        "train_all_path = os.path.join(dataset_path, 'train_all.json')\n",
        "\n",
        "# collate_fn needs for batch\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_transform = A.Compose([  \n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.9),\n",
        "    A.RandomGamma(p=0.3),\n",
        "    A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, shift_limit=0.1, p=1),\n",
        "    # A.OneOf([\n",
        "    #   A.RandomContrast(limit=0.1),\n",
        "    #   A.RandomGamma(),\n",
        "    #   A.RandomBrightness(limit=0.1),\n",
        "    #   ], p=0.9),\n",
        "    A.Cutout(num_holes=1, max_h_size=60, max_w_size=60),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = A.Compose([  \n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "# test dataset\n",
        "train_all_dataset = CustomDataset(data_dir=train_all_path, mode='train', transform=train_transform)\n",
        "test_dataset = CustomDataset(data_dir=test_path, mode='test', transform=test_transform)\n",
        "dataset = torch.utils.data.ConcatDataset([train_all_dataset, test_dataset])\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
        "                                           batch_size=8,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True,\n",
        "                                           num_workers=4,\n",
        "                                           drop_last=True,\n",
        "                                           collate_fn=collate_fn)\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "#                                           batch_size=8,\n",
        "#                                           pin_memory=True,\n",
        "#                                           num_workers=4,\n",
        "#                                           collate_fn=collate_fn)\n",
        "\n",
        "# test_tta_loader = torch.utils.data.DataLoader(dataset=test_tta_dataset,\n",
        "#                                           batch_size=3,\n",
        "#                                           pin_memory=True,\n",
        "#                                           num_workers=4,\n",
        "#                                           collate_fn=collate_fn)\n",
        "\n",
        "# test_vtta_loader = torch.utils.data.DataLoader(dataset=test_vtta_dataset,\n",
        "#                                           batch_size=3,\n",
        "#                                           pin_memory=True,\n",
        "#                                           num_workers=4,\n",
        "#                                           collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=13.34s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.06s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx27yyQ-kMXf"
      },
      "source": [
        "for imgs, masks, image_infos in train_loader:\n",
        "    image_infos = image_infos[0]\n",
        "    temp_images = imgs\n",
        "    temp_masks = masks\n",
        "    break\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 12))\n",
        "\n",
        "print('image shape:', list(temp_images[0].shape))\n",
        "print('mask shape: ', list(temp_masks[0].shape))\n",
        "print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(temp_masks[0]))])\n",
        "\n",
        "ax1.imshow(temp_images[0].permute([1,2,0]))\n",
        "ax1.grid(False)\n",
        "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
        "\n",
        "ax2.imshow(temp_masks[0])\n",
        "ax2.grid(False)\n",
        "ax2.set_title(\"masks : {}\".format(image_infos['file_name']), fontsize = 15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTaWJxrZn_BC"
      },
      "source": [
        "def train(num_epochs, model, data_loader, criterion,  optimizer, scheduler, saved_dir, i, device):\n",
        "  print('Start training..')\n",
        "  best_loss = 9999999\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      trn_mIoU = []\n",
        "      trn_acc = []\n",
        "      model.zero_grad()       \n",
        "      for step, (images, masks, _) in enumerate(data_loader):\n",
        "          images = torch.stack(images)       # (batch, channel, height, width)\n",
        "          masks = torch.stack(masks).long()  # (batch, height, width)\n",
        "          masks_tensor = masks.view(images.shape[0], 1, images.shape[2], images.shape[3])\n",
        "          zeros = torch.zeros(images.shape[0], 12, images.shape[2], images.shape[3], dtype=masks.dtype)\n",
        "          masks = zeros.scatter_(1, masks_tensor, 1).to(device) \n",
        "          images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, masks) \n",
        "          loss = loss / 4\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "          if (step+1) % 4 == 0:             \n",
        "            optimizer.step()                            \n",
        "            model.zero_grad()    \n",
        "\n",
        "          outputs = torch.argmax(outputs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "          masks = torch.argmax(masks.squeeze(), dim=1).detach().cpu().numpy()\n",
        "          res = label_accuracy_score(masks, outputs, n_class=12)\n",
        "          # tmIoU_list, tb_mIoU = mIoU(outputs, masks, smooth=1e-10, n_classes=12)\n",
        "          # acc = pixel_accuracy(outputs, masks)\n",
        "          trn_mIoU.append(res[2])\n",
        "          trn_acc.append(res[0])\n",
        "          \n",
        "          # print the loss at 20 step intervals.\n",
        "          if (step + 1) % 20 == 0:\n",
        "              print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, lr: {}'.format(\n",
        "                  epoch+1, num_epochs, step+1, len(train_loader), loss.item(), optimizer.param_groups[0][\"lr\"]))\n",
        "      print('Epoch {} - mIoU: {:.4f}, acc: {:.4f}'.format(epoch+1, np.mean(trn_mIoU), np.mean(trn_acc)))\n",
        "      scheduler.step()\n",
        "  save_model(model, saved_dir, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HujQBGUZsbil"
      },
      "source": [
        "# define the evaluation function\n",
        "# https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/utils.py\n",
        "import numpy as np\n",
        "\n",
        "def _fast_hist(label_true, label_pred, n_class):\n",
        "    mask = (label_true >= 0) & (label_true < n_class)\n",
        "    hist = np.bincount(\n",
        "        n_class * label_true[mask].astype(int) +\n",
        "        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n",
        "    return hist\n",
        "\n",
        "\n",
        "def label_accuracy_score(label_trues, label_preds, n_class):\n",
        "    \"\"\"Returns accuracy score evaluation result.\n",
        "      - overall accuracy\n",
        "      - mean accuracy\n",
        "      - mean IU\n",
        "      - fwavacc\n",
        "    \"\"\"\n",
        "    hist = np.zeros((n_class, n_class))\n",
        "    for lt, lp in zip(label_trues, label_preds):\n",
        "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
        "    acc = np.diag(hist).sum() / hist.sum()\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
        "    acc_cls = np.nanmean(acc_cls)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        iu = np.diag(hist) / (\n",
        "            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n",
        "        )\n",
        "    mean_iu = np.nanmean(iu)\n",
        "    freq = hist.sum(axis=1) / hist.sum()\n",
        "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "    return acc, acc_cls, mean_iu, fwavacc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzhbmEOHsgbY"
      },
      "source": [
        "saved_dir = '/content/drive/MyDrive/segment/saved/pseudo'\n",
        "if not os.path.isdir(saved_dir):                                                           \n",
        "    os.mkdir(saved_dir)\n",
        "    \n",
        "def save_model(model, saved_dir, i, file_name='best_model_withcond.pt'):\n",
        "    file_name = str(i) + file_name\n",
        "    check_point = {'net': model.state_dict()}\n",
        "    output_path = os.path.join(saved_dir, file_name)\n",
        "    torch.save(model, output_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjYHLMKtsqj1"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "try:\n",
        "    from itertools import ifilterfalse\n",
        "except ImportError:  # py3k\n",
        "    from itertools import filterfalse\n",
        "\n",
        "eps = 1e-6\n",
        "\n",
        "\n",
        "def dice_round(preds, trues):\n",
        "    preds = preds.float()\n",
        "    return soft_dice_loss(preds, trues)\n",
        "\n",
        "\n",
        "def soft_dice_loss(outputs, targets, per_image=False):\n",
        "    batch_size = outputs.size()[0]\n",
        "    eps = 1e-5\n",
        "    if not per_image:\n",
        "        batch_size = 1\n",
        "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
        "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) + eps\n",
        "    loss = (1 - (2 * intersection + eps) / union).mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def jaccard(outputs, targets, per_image=False, non_empty=False, min_pixels=5):\n",
        "    batch_size = outputs.size()[0]\n",
        "    eps = 1e-3\n",
        "    if not per_image:\n",
        "        batch_size = 1\n",
        "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
        "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
        "    target_sum = torch.sum(dice_target, dim=1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    losses = 1 - (intersection + eps) / (torch.sum(dice_output + dice_target, dim=1) - intersection + eps)\n",
        "    if non_empty:\n",
        "        assert per_image == True\n",
        "        non_empty_images = 0\n",
        "        sum_loss = 0\n",
        "        for i in range(batch_size):\n",
        "            if target_sum[i] > min_pixels:\n",
        "                sum_loss += losses[i]\n",
        "                non_empty_images += 1\n",
        "        if non_empty_images == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return sum_loss / non_empty_images\n",
        "\n",
        "    return losses.mean()\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return soft_dice_loss(input, target, per_image=self.per_image)\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False, non_empty=False, apply_sigmoid=False,\n",
        "                 min_pixels=5):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "        self.non_empty = non_empty\n",
        "        self.apply_sigmoid = apply_sigmoid\n",
        "        self.min_pixels = min_pixels\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if self.apply_sigmoid:\n",
        "            input = torch.sigmoid(input)\n",
        "        return jaccard(input, target, per_image=self.per_image, non_empty=self.non_empty, min_pixels=self.min_pixels)\n",
        "\n",
        "\n",
        "class StableBCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StableBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input = input.float().contiguous().view(-1)\n",
        "        target = target.float().contiguous().view(-1)\n",
        "        neg_abs = - input.abs()\n",
        "        # todo check correctness\n",
        "        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
        "        return loss.mean()\n",
        "\n",
        "class FocalLoss2d(nn.Module):\n",
        "    def __init__(self, gamma=2, ignore_index=255):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        eps = 1e-8\n",
        "        non_ignored = targets.contiguous().view(-1) != self.ignore_index\n",
        "        targets = targets.contiguous().view(-1)[non_ignored].float()\n",
        "        outputs = outputs.contiguous().view(-1)[non_ignored]\n",
        "        outputs = torch.clamp(outputs, eps, 1. - eps)\n",
        "        targets = torch.clamp(targets, eps, 1. - eps)\n",
        "        pt = (1 - targets) * (1 - outputs) + targets * outputs\n",
        "        return (-(1. - pt) ** self.gamma * torch.log(pt)).mean()\n",
        "\n",
        "class ComboLoss(nn.Module):\n",
        "    def __init__(self, weights, per_image=False, channel_weights=[1, 0.5, 0.5], channel_losses=None):\n",
        "        super().__init__()\n",
        "        self.weights = weights\n",
        "        self.bce = StableBCELoss()\n",
        "        self.dice = DiceLoss(per_image=False)\n",
        "        self.jaccard = JaccardLoss(per_image=False)\n",
        "        self.focal = FocalLoss2d()\n",
        "        self.mapping = {'bce': self.bce,\n",
        "                        'dice': self.dice,\n",
        "                        'focal': self.focal,\n",
        "                        'jaccard': self.jaccard}\n",
        "        self.expect_sigmoid = {'dice', 'focal', 'jaccard'}\n",
        "        self.per_channel = {'dice', 'jaccard'}\n",
        "        self.values = {}\n",
        "        self.channel_weights = channel_weights\n",
        "        self.channel_losses = channel_losses\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = 0\n",
        "        weights = self.weights\n",
        "        sigmoid_input = torch.sigmoid(outputs)\n",
        "        for k, v in weights.items():\n",
        "            if not v:\n",
        "                continue\n",
        "            val = 0 \n",
        "            if k in self.per_channel:\n",
        "                channels = targets.size(1)\n",
        "                for c in range(channels):\n",
        "                    if not self.channel_losses or k in self.channel_losses[c]:\n",
        "                        val += self.channel_weights[c] * self.mapping[k](sigmoid_input[:, c, ...] if k in self.expect_sigmoid else outputs[:, c, ...],\n",
        "                                               targets[:, c, ...])\n",
        "\n",
        "            else:\n",
        "                val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n",
        "            self.values[k] = val\n",
        "            loss += self.weights[k] * val\n",
        "            # print(k, val)\n",
        "        return loss.clamp(min=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhZ7iDMwssYq"
      },
      "source": [
        "## Over9000 Optimizer . Inspired by Iafoss . Over and Out !\n",
        "##https://github.com/mgrankin/over9000/blob/master/ralamb.py\n",
        "import torch, math\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "# RAdam + LARS\n",
        "class Ralamb(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-2, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(Ralamb, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Ralamb, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Ralamb does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # m_t\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                # v_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, radam_step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = radam_step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                radam_step = p_data_fp32.clone()\n",
        "                if N_sma >= 5:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n",
        "                else:\n",
        "                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n",
        "\n",
        "                radam_norm = radam_step.pow(2).sum().sqrt()\n",
        "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
        "                if weight_norm == 0 or radam_norm == 0:\n",
        "                    trust_ratio = 1\n",
        "                else:\n",
        "                    trust_ratio = weight_norm / radam_norm\n",
        "\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = radam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "\n",
        "                if N_sma >= 5:\n",
        "                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Lookahead implementation from https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/lookahead.py\n",
        "\n",
        "\"\"\" Lookahead Optimizer Wrapper.\n",
        "Implementation modified from: https://github.com/alphadl/lookahead.pytorch\n",
        "Paper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from collections import defaultdict\n",
        "\n",
        "class Lookahead(Optimizer):\n",
        "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
        "        self.base_optimizer = base_optimizer\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults = base_optimizer.defaults\n",
        "        self.defaults.update(defaults)\n",
        "        self.state = defaultdict(dict)\n",
        "        # manually add our defaults to the param groups\n",
        "        for name, default in defaults.items():\n",
        "            for group in self.param_groups:\n",
        "                group.setdefault(name, default)\n",
        "\n",
        "    def update_slow(self, group):\n",
        "        for fast_p in group[\"params\"]:\n",
        "            if fast_p.grad is None:\n",
        "                continue\n",
        "            param_state = self.state[fast_p]\n",
        "            if 'slow_buffer' not in param_state:\n",
        "                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
        "                param_state['slow_buffer'].copy_(fast_p.data)\n",
        "            slow = param_state['slow_buffer']\n",
        "            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
        "            fast_p.data.copy_(slow)\n",
        "\n",
        "    def sync_lookahead(self):\n",
        "        for group in self.param_groups:\n",
        "            self.update_slow(group)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        # print(self.k)\n",
        "        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n",
        "        loss = self.base_optimizer.step(closure)\n",
        "        for group in self.param_groups:\n",
        "            group['lookahead_step'] += 1\n",
        "            if group['lookahead_step'] % group['lookahead_k'] == 0:\n",
        "                self.update_slow(group)\n",
        "        return loss\n",
        "\n",
        "    def state_dict(self):\n",
        "        fast_state_dict = self.base_optimizer.state_dict()\n",
        "        slow_state = {\n",
        "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
        "            for k, v in self.state.items()\n",
        "        }\n",
        "        fast_state = fast_state_dict['state']\n",
        "        param_groups = fast_state_dict['param_groups']\n",
        "        return {\n",
        "            'state': fast_state,\n",
        "            'slow_state': slow_state,\n",
        "            'param_groups': param_groups,\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        fast_state_dict = {\n",
        "            'state': state_dict['state'],\n",
        "            'param_groups': state_dict['param_groups'],\n",
        "        }\n",
        "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
        "\n",
        "        # We want to restore the slow state, but share param_groups reference\n",
        "        # with base_optimizer. This is a bit redundant but least code\n",
        "        slow_state_new = False\n",
        "        if 'slow_state' not in state_dict:\n",
        "            print('Loading state_dict from optimizer without Lookahead applied.')\n",
        "            state_dict['slow_state'] = defaultdict(dict)\n",
        "            slow_state_new = True\n",
        "        slow_state_dict = {\n",
        "            'state': state_dict['slow_state'],\n",
        "            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
        "        }\n",
        "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
        "        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
        "        if slow_state_new:\n",
        "            # reapply defaults to catch missing lookahead specific ones\n",
        "            for name, default in self.defaults.items():\n",
        "                for group in self.param_groups:\n",
        "                    group.setdefault(name, default)\n",
        "\n",
        "def LookaheadAdam(params, alpha=0.5, k=6, *args, **kwargs):\n",
        "     adam = Adam(params, *args, **kwargs)\n",
        "     return Lookahead(adam, alpha, k)\n",
        "\n",
        "\n",
        "# RAdam + LARS + LookAHead\n",
        "\n",
        "# Lookahead implementation from https://github.com/lonePatient/lookahead_pytorch/blob/master/optimizer.py\n",
        "# RAdam + LARS implementation from https://gist.github.com/redknightlois/c4023d393eb8f92bb44b2ab582d7ec20\n",
        "\n",
        "def Over9000(params, alpha=0.5, k=6, *args, **kwargs):\n",
        "     ralamb = Ralamb(params, *args, **kwargs)\n",
        "     return Lookahead(ralamb, alpha, k)\n",
        "\n",
        "RangerLars = Over9000 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVQR2G7kst2Z"
      },
      "source": [
        "criterion = ComboLoss(weights={'bce': 5,'dice': 1,'focal': 5},\n",
        "                      channel_weights=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01])\n",
        "                      # channel_weights=[0.0001, 0.1813306 , 0.01049447, 0.00313614, 0.04424467, 0.05193036, 0.04788384, 0.00945399, 0.02173117, 0.00382078, 0.46088194, 0.16509204],\n",
        "                      # channel_losses=0)\n",
        "optimizer = Over9000(model.parameters(), lr=1e-7, weight_decay=1e-4) \n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-8, last_epoch=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs1BurGSswJk"
      },
      "source": [
        "i = 0\n",
        "for model in models[:2]:\n",
        "  train(10, model, train_loader, criterion, optimizer, scheduler, saved_dir, i, device)\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwUGxh7opSfs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}